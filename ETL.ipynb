{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Tracker Raw Data ETL (DEV).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "k3RiM020XzOP",
        "QgZxeGE3UXTQ",
        "72hli7z-iyrK",
        "l7XJrEYfawzH",
        "615TnzfTXG4L",
        "euE3ubGA02W4",
        "j_OBxeoL2jZb",
        "tx1lF8_bs3d-",
        "t0tGAt4b6p0E",
        "R8k3J3yIIwTq",
        "3iXKF4gmqD3o",
        "whgN1pE3hRI_",
        "IO4L7X3mvHS0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teQxkGjTuZak"
      },
      "source": [
        "# Upload file from GCS to Survey"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlrWeSwOd-gd"
      },
      "source": [
        "## Initial Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp4T-AgQGM46"
      },
      "source": [
        "#### Connect to GCP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBXREhZVGLhG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaa72bb6-5cc5-4946-d528-8d6c910cf284"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "print('Authenticated')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-XJIS7QGTxE"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "import pandas_gbq as gbq\n",
        "\n",
        "project='dummydummy'\n",
        "client = bigquery.Client(project = project)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5vWjUMLWIOA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05730680-d0f0-4a49-9cb5-43cb49f24489"
      },
      "source": [
        "%env GOOGLE_CLOUD_PROJECT=dummydummy-dummydummy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN0O561Od1og"
      },
      "source": [
        "#### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KnycUQDkWgj"
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "import re\n",
        "from google.colab import files\n",
        "from time import strptime\n",
        "import datetime\n",
        "import numpy as np\n",
        "from google.cloud import storage\n",
        "#data  = pd.read_excel (io.BytesIO(uploaded[filename]))\n",
        "\n",
        "#print(data.sheet_names)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F--dAwfa6IJp"
      },
      "source": [
        "#### Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA1U8SkyfHzI"
      },
      "source": [
        "# This function runs a query and returns the results.\n",
        "# We use it to ensure we don't run too much in parallel\n",
        "\n",
        "def exec_query(q):\n",
        "  r1 = client.query(q)\n",
        "  if r1.errors is None:\n",
        "      r2 = r1.result()\n",
        "  else:\n",
        "      raise Exception(r1.error_result)\n",
        "  return r1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30i4AQ-w6Kw2"
      },
      "source": [
        "# reading an xlsx file - raw data\n",
        "def reading_file_from_bucket (filename, bucket_name = \"dummydummy\"):\n",
        "  src = f\"'gs://{bucket_name}/{filename}'\"\n",
        "  !gsutil -m cp {src} .\n",
        "  xl = pd.ExcelFile(filename)\n",
        "  return xl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW-qJy9gbQ1T"
      },
      "source": [
        "def archive_processed_file(filename, bucket_name = \"dummydummy\"):\n",
        "  src = f\"\"\"'gs://{bucket_name}/{filename}'\"\"\"\n",
        "  to_scr = f\"\"\"'gs://{bucket_name}/Archive/'\"\"\"\n",
        "  !gsutil mv {src} {to_scr}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sHQEdtg8Wws"
      },
      "source": [
        "# extract dummydummy year from file name\n",
        "def get_survey_year(filename):\n",
        "  survey_year = re.findall('[a-zA-Z]{3,4}[ |_]([0-9]{4})', filename)[0]\n",
        "  return survey_year"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgZY1p3F81W1"
      },
      "source": [
        "# extract dummydummy month from file name\n",
        "def get_survey_month(filename):\n",
        "  survey_month = strptime(re.findall('([a-zA-Z]{3,4})[ |_][0-9]{4}', filename)[0][:3],'%b').tm_mon\n",
        "  return survey_month"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYtZdv6sTVz6"
      },
      "source": [
        "def get_parallel_tracker_txt(filename):\n",
        "  if len(re.findall('(?i)parallel', filename)) == 0:\n",
        "    parallel_id = ''\n",
        "  else: \n",
        "    parallel_id = 'parallel_'\n",
        "  return parallel_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kTzJPsu9Y8b"
      },
      "source": [
        "def push_raw_data_to_bq(xl, stg_dateset = 'dev_stg'):\n",
        "  ds1 = xl.parse('Data')\n",
        "  ds1.insert(0, \"response_id\",'', allow_duplicates =False)\n",
        "  survey_year = get_survey_year(filename)\n",
        "  survey_month = get_survey_month(filename)\n",
        "  parallel_id = get_parallel_tracker_txt(filename)\n",
        "\n",
        "  for index, row in ds1.iterrows():\n",
        "    ds1.loc[index,'response_id'] = 'dummydummy' + str(parallel_id) + str(row['PID']) + '_' + str(row['INTERVIEW_START'])[0:10] + '_'+ str(row['STATUS'])\n",
        "    ds1.loc[index,'audience_id'] = str(parallel_id) + str(row['PID'])\n",
        "    ds1.loc[index,'survey_date'] = datetime.date(pd.to_numeric(dummydummy),survey_month,1)\n",
        "\n",
        "  result = pd.melt(ds1, id_vars=['response_id', 'audience_id', 'STATUS','survey_date'] ).sort_values(['response_id', 'audience_id'])\n",
        "  result['variable'] = str(parallel_id) + result['variable']\n",
        "  gbq.to_gbq(result,f\"{stg_dateset}.response_data\", project, if_exists='replace')\n",
        "  return result\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTIqRzspmBkq"
      },
      "source": [
        "def merge_into_response(dataset, stg_dateset = 'dev_stg'):\n",
        "  q = f\"\"\"\n",
        "  MERGE INTO {dataset}.response AS r\n",
        "  USING {stg_dateset}.response_data t\n",
        "  ON r.response_id = t.response_id and r.question_id = concat('dummydummy' , safe_cast(t.variable as string))\n",
        "  WHEN NOT MATCHED THEN\n",
        "    INSERT(\n",
        "    response_id, \n",
        "    question_id,\n",
        "    audience_id,\n",
        "    supplier_name,\n",
        "    supplier_respondent_id,\n",
        "    supplier_question_id,\n",
        "    text,\n",
        "    created_at,\n",
        "    updated_at,\n",
        "    is_valid,\n",
        "    survey_date)\n",
        "    VALUES( concat(safe_cast(response_id as string)) ,\n",
        "            concat('dummydummy' , safe_cast(variable as string)),\n",
        "            concat('dummydummy' , safe_cast(audience_id as string)),\n",
        "            'dummydummy',\n",
        "            audience_id,\n",
        "            variable,\n",
        "            value,\n",
        "            current_datetime(\"Pacific/Auckland\"),\n",
        "            current_datetime(\"Pacific/Auckland\"),\n",
        "            if(safe_cast(STATUS as string) = '1',true,false),\n",
        "            safe_cast(survey_date as date)\n",
        "          )\n",
        "  WHEN MATCHED THEN\n",
        "    UPDATE SET\n",
        "              r.audience_id = concat('dummydummy' , safe_cast(t.audience_id as string)),\n",
        "              r.supplier_name = 'dummydummy',\n",
        "              r.supplier_respondent_id = t.audience_id,\n",
        "              r.supplier_question_id = t.variable,\n",
        "              r.text = t.value,\n",
        "              r.updated_at = current_datetime(\"Pacific/Auckland\"),\n",
        "              r.is_valid = if(safe_cast(STATUS as string) = '1',true,false),\n",
        "              r.survey_date = safe_cast(t.survey_date as date)\"\"\"\n",
        "  r = exec_query(q)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpx5SBy-sulS"
      },
      "source": [
        "# step 1: load question data from google sheet\n",
        "def question_override(dataset, stg_dateset = 'dev_stg', master = 'master_sheet'):\n",
        "  q = f\"\"\"\n",
        "      MERGE INTO {dataset}.question AS q\n",
        "      USING {master}.dummydummy m\n",
        "      ON q.question_id = concat('dummydummy', safe_cast(m.supplier_question_id as string))\n",
        "      WHEN NOT MATCHED THEN\n",
        "        INSERT(question_id, \n",
        "              survey_id,\n",
        "              supplier_question_id, \n",
        "              page_heading, \n",
        "              page_sub_heading, \n",
        "              question_type, \n",
        "              question_tag, \n",
        "              question_tag_display_in_interface,\n",
        "              created_at,\n",
        "              created_by,\n",
        "              updated_at,\n",
        "              updated_by,\n",
        "              supplier_name,\n",
        "              index_type,\n",
        "              index_weighting,\n",
        "              index_rank)\n",
        "        VALUES(concat('dummydummy', safe_cast(supplier_question_id as string)),\n",
        "              'dummydummy',\n",
        "              supplier_question_id, \n",
        "              page_heading, \n",
        "              REGEXP_REPLACE(page_sub_heading, \"'\", \" \"), \n",
        "              question_type, \n",
        "              question_tag, \n",
        "              question_tag_display_in_interface,\n",
        "              current_datetime(\"Pacific/Auckland\"),\n",
        "              'dummydummy',\n",
        "              current_datetime(\"Pacific/Auckland\"),\n",
        "              'dummydummy',\n",
        "              'dummydummy',\n",
        "              index_type,\n",
        "              index_weighting,\n",
        "              index_rank\n",
        "              )\n",
        "      WHEN MATCHED THEN\n",
        "        UPDATE SET q.page_heading = m.page_heading,\n",
        "                  q.page_sub_heading = REGEXP_REPLACE(m.page_sub_heading, \"'|’\", \" \"),\n",
        "                  q.question_type = m.question_type,\n",
        "                  q.question_tag = m.question_tag,\n",
        "                  q.question_tag_display_in_interface = m.question_tag_display_in_interface,\n",
        "                  q.index_type = m.index_type,\n",
        "                  q.index_weighting = m.index_weighting,\n",
        "                  q.index_rank = m.index_rank,\n",
        "                  q.updated_at = current_datetime(\"Pacific/Auckland\")\n",
        "      \"\"\"\n",
        "  exec_query(q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thrjOUh5ub6w"
      },
      "source": [
        "# process questions\n",
        "def load_questions_from_xl(data, stg_dateset = 'dev_stg'):\n",
        "  parallel_id = get_parallel_tracker_txt(filename)\n",
        "  question_df = data.parse('Labels', skiprows=1)\n",
        "  question_df = question_df.dropna()\n",
        "  question_df['Variable'] = str(parallel_id) + question_df['Variable']\n",
        "  question_df['question_text'] = question_df['Label'].replace(['^[A-Z0-9_]+ '], [''], regex=True)\n",
        "  question_df['page_heading'] = question_df['Label'].str.findall(r'^([A-Z0-9_]+) ').apply(', '.join)\n",
        "  separator = \"\\. |\\? \"\n",
        "  ls = question_df['question_text'].str.split(separator).tolist()\n",
        "  sub_page_separator = \" - \"\n",
        "  question_df['page'] = [c[-1] for c in ls]\n",
        "  question_df['sub_page'] = question_df['page'].str.findall(r\"- (.*)\").apply(''.join)\n",
        "  question_df['page'] = question_df['page'].replace([' - .*$'],[''], regex = True)\n",
        "  gbq.to_gbq(question_df,f'{stg_dateset}.dummydummy', project, if_exists='replace')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFHoWN3XwRF1"
      },
      "source": [
        "# insert_new_questions\n",
        "def insert_new_questions(dataset, stg_dateset = 'dev_stg'):\n",
        "  q = f\"\"\"\n",
        "      MERGE INTO {dataset}.question AS q\n",
        "      USING {stg_dateset}.dummydummy n\n",
        "      ON q.question_id = concat('dummydummy', safe_cast(n.Variable as string))\n",
        "      WHEN NOT MATCHED THEN\n",
        "        INSERT(question_id, page_heading, page_sub_heading, survey_id, question_text, created_at, created_by, updated_at, updated_by, supplier_page_id, supplier_question_id, supplier_name, question_tag_display_in_interface)\n",
        "        VALUES(concat('dummydummy', safe_cast(Variable as string)), \n",
        "                page, \n",
        "                sub_page,\n",
        "                'dummydummy',\n",
        "                question_text, \n",
        "                current_datetime(\"Pacific/Auckland\"),\n",
        "                'dummydummy',\n",
        "                current_datetime(\"Pacific/Auckland\"),\n",
        "                'dummydummy',\n",
        "                page_heading, \n",
        "                safe_cast(Variable as string),\n",
        "                'dummydummy',\n",
        "                True\n",
        "              )\n",
        "              \"\"\"\n",
        "  exec_query(q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haF91Vagxy6U"
      },
      "source": [
        "# load audience into BQ\n",
        "def load_raw_audience(dataset, response, stg_dateset='dev_stg'):\n",
        "  col_keep = [\n",
        "       'response_id','RESPID','STATUS','PID','survey_date',\n",
        "       'S2', # gender\n",
        "       'S3', # age\n",
        "       'S4', # region/area\n",
        "       'A3_1','A3_2','A3_3', # shopper type. e.g. non shopper, only shopper\n",
        "       'C2_1','C2_2','C2_3','C2_4','C2_5','C2_6','C2_7','C2_8','C2_9', # main shopper\n",
        "       'CX_1','CX_2','CX_3', # own shopper\n",
        "       'X2AA_4','X12A_1','X12A_2','X12A_3','X12A_4','X12A_5', # life-stage\n",
        "       'E11', # Online Flag\n",
        "       'D10_1', # reward I value\n",
        "       'D10C',\n",
        "       ]\n",
        "  question_df = exec_query(f\"\"\"select * from {dataset}.question\"\"\").to_dataframe()\n",
        "  a_df = response[response.variable.isin(question_df[question_df['question_type'] == 'DUMMY'].supplier_question_id)]\n",
        "  a_df = a_df[a_df.STATUS == 1]\n",
        "  a_df['variable'] = a_df['variable'].str.replace('parallel_', '')\n",
        "  audience_p = a_df.pivot(index='response_id', columns='variable', values='value').rename_axis(None)\n",
        "  audience_p.reset_index(inplace = True)\n",
        "  for c in col_keep:\n",
        "   if c not in audience_p.columns:\n",
        "     audience_p[c] = None\n",
        "  gbq.to_gbq(audience_p,f\"\"\"{stg_dateset}.dummydummy\"\"\", project, if_exists='replace')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DfIroNm2Y_1"
      },
      "source": [
        "## create dummydummy dummydummy audience file for reporting\n",
        "def create_audience_table(dataset, stg_dateset='dev_stg'):\n",
        "  q = f\"\"\"\n",
        "  create or replace table {stg_dateset}.dummydummy as\n",
        "  With Audience as\n",
        "  (\n",
        "  SELECT  \n",
        "    index as response_id,\n",
        "    case SAFE_CAST(S2 AS NUMERIC) when 1 then 'MALE' when 2 then 'FEMALE' else null end as gender,\n",
        "\n",
        "        CASE \n",
        "dummydummy\n",
        "dummydummy\n",
        "dummydummy\n",
        "          ELSE NULL\n",
        "        END as age,\n",
        "       \n",
        "       dummydummy\n",
        "       dummydummy\n",
        "       dummydummy\n",
        "              else null end as region,\n",
        "              \n",
        "        CASE WHEN SAFE_CAST(S4 AS NUMERIC) = 2 then 'AUCKLAND'\n",
        "              dummydummy\n",
        "              dummydummy\n",
        "              dummydummy\n",
        "              else null end as area,          \n",
        "\n",
        "                  CASE \n",
        "                      dummydummy\n",
        "                      dummydummy\n",
        "                      dummydummy\n",
        "                      dummydummy\n",
        "                      END as lifestage_segment,\n",
        "    case SAFE_CAST(E11 AS NUMERIC) when 1 then 1 else 0 end as dummydummy,\n",
        "    \n",
        "    case SAFE_CAST(CX_1 AS NUMERIC) when 1 then 1 else 0 end as dummydummy,\n",
        "    case SAFE_CAST(CX_3 AS NUMERIC) when 1 then 1 else 0 end as dummydummy,\n",
        "    case SAFE_CAST(CX_2 AS NUMERIC) when 1 then 1 else 0 end as dummydummy,\n",
        "    \n",
        "    case SAFE_CAST(C2_1 AS NUMERIC) when 5 then 1 else 0 end as dummydummy,\n",
        "    case SAFE_CAST(C2_3 AS NUMERIC) when 5 then 1 else 0 end as dummydummy,\n",
        "    case SAFE_CAST(C2_2 AS NUMERIC) when 5 then 1 else 0 end as dummydummy,\n",
        "    \n",
        "    case when SAFE_CAST(CX_1 AS NUMERIC) = 1 and SAFE_CAST(CX_3 AS NUMERIC) = 1  then 1 else 0 end as dummydummy,\n",
        "    case when SAFE_CAST(CX_1 AS NUMERIC) = 1 and SAFE_CAST(CX_2 AS NUMERIC) = 1  then 1 else 0 end as dummydummy,\n",
        "    SAFE_CAST(D10_1 AS INT64) as dummydummy,\n",
        "    case when SAFE_CAST(D10C AS INT64) = 1 then 'Food'\n",
        "         when SAFE_CAST(D10C AS INT64) = 2 then 'Fuel'\n",
        "         else 'None' end as dummydummy,\n",
        "  FROM {stg_dateset}.dummydummy\n",
        "  )\n",
        "  ,\n",
        "  Quota as\n",
        "  (\n",
        "  Select \n",
        "    gender,\n",
        "    area,\n",
        "    age,\n",
        "    target\n",
        "  from {dataset}.dummydummy\n",
        "  )\n",
        "  Select \n",
        "  response_id, Audience.gender, Audience.age, region, Audience.area, lifestage_segment, online_shopper, CD_own_shopper, NW_own_shopper, PNS_own_shopper, CD_main_shopper, NW_main_shopper, PNS_main_shopper, cross_shopper_CD_NW, cross_shopper_CD_PNS, Onecard_Reward_I_Value, onecard_reward_type,\n",
        "  target / (sum(1) over (partition by Audience.area,Audience.age,Audience.gender)/sum(1) over ()) as weight,\n",
        "  --sum(1) over (partition by Audience.area,Audience.age,Audience.gender) as group_count_temp,\n",
        "  --sum(1) as monthly_count\n",
        "  from Audience \n",
        "  left join Quota on concat(Quota.gender, Quota.age,Quota.area) = concat(dummydummy.gender, dummydummy.age,dummydummy.area)\n",
        "  \"\"\"\n",
        "  r = exec_query(q)\n",
        "\n",
        "  insert_q = f\"\"\"\n",
        "  MERGE INTO {dataset}.dummydummy AS a\n",
        "  USING {stg_dateset}.dummydummy t\n",
        "  ON a.response_id = t.response_id\n",
        "  WHEN NOT MATCHED THEN\n",
        "  INSERT(response_id, gender, age, region, area, lifestage_segment, \n",
        "        online_shopper, dummydummy, dummydummy, PNS_own_shopper, dummydummy, \n",
        "        dummydummy, dummydummy, dummydummy, dummydummy, \n",
        "        Onecard_Reward_I_Value, weight, onecard_reward_type)\n",
        "  VALUES(response_id, gender, age, region, area, lifestage_segment, \n",
        "        online_shopper, dummydummy, dummydummy, dummydummy, dummydummy, \n",
        "        dummydummy, dummydummy, dummydummy, dummydummy, \n",
        "        dummydummy, weight, dummydummy\n",
        "         ) \n",
        "  WHEN MATCHED THEN\n",
        "  UPDATE SET a.gender = t.gender,\n",
        "             a.age = t.age,\n",
        "             a.region = t.region,\n",
        "             a.area = t.area,\n",
        "             a.dummydummy = t.dummydummy,\n",
        "             a.online_shopper = t.online_shopper,\n",
        "             a.CD_own_shopper = t.CD_own_shopper,\n",
        "             a.NW_own_shopper = t.NW_own_shopper,\n",
        "             a.PNS_own_shopper = t.PNS_own_shopper,\n",
        "             a.CD_main_shopper = t.CD_main_shopper,\n",
        "             a.NW_main_shopper = t.NW_main_shopper,\n",
        "             a.PNS_main_shopper = t.PNS_main_shopper,\n",
        "             a.cross_shopper_CD_NW = t.cross_shopper_CD_NW,\n",
        "             a.cross_shopper_CD_PNS = t.cross_shopper_CD_PNS,\n",
        "             a.dummydummy = t.dummydummy,\n",
        "             a.weight = t.weight,\n",
        "             a.onecard_reward_type = t.onecard_reward_type\n",
        "  \"\"\"\n",
        "  r = exec_query(insert_q)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QzeRVbs4k9E"
      },
      "source": [
        "def create_table_for_dashboard(dataset):\n",
        "  q = f\"\"\"\n",
        "      create or replace table dummydummy.dummydummy as\n",
        "      with t1 as\n",
        "      (\n",
        "        SELECT \n",
        "        aud.*,\n",
        "        res.survey_date,\n",
        "        res.question_id,\n",
        "        q.supplier_question_id,\n",
        "        q.question_text,\n",
        "        q.page_heading ,\n",
        "        q.page_sub_heading ,\n",
        "        q.index_type,\n",
        "        q.index_weighting,\n",
        "        q.index_rank,\n",
        "        res.text,\n",
        "      FROM {dataset}.dummydummy aud\n",
        "      inner join {dataset}.response res on res.response_id = aud.response_id\n",
        "      inner join {dataset}.question q on q.question_id = res.question_id \n",
        "      where 1 = 1\n",
        "        and q.question_id is not null\n",
        "        and res.text is not null\n",
        "        and res.text <> ' '\n",
        "        and q.question_tag_display_in_interface\n",
        "        and (q.question_type <> 'IGNORE' or q.question_type is null)\n",
        "      )\n",
        "      ,\n",
        "      t2 as\n",
        "      (\n",
        "      select\n",
        "      max(survey_date) reporting_month\n",
        "      from t1\n",
        "      )\n",
        "      select\n",
        "      *,\n",
        "      date_diff(reporting_month, survey_date, Month) month_rank\n",
        "      from t1,t2\n",
        "      \"\"\"\n",
        "  r = exec_query(q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC2PRlL-XxMq"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2INf8dq_5qVS"
      },
      "source": [
        "## Processing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx3qRhfjaMME",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93569a72-4708-4d9f-8681-cadd0fffffc1"
      },
      "source": [
        "gcs_client = storage.Client()\n",
        "bucket = gcs_client.bucket('dummydummy')\n",
        "for blob in bucket.list_blobs(prefix='ORD'):\n",
        "  filename = blob.name\n",
        "  xl = reading_file_from_bucket (filename, bucket_name = \"dummydummy\")\n",
        "  print('file loaded')\n",
        "  response = push_raw_data_to_bq(xl)\n",
        "  merge_into_response(dataset = 'dev_survey', stg_dateset = 'dev_stg')\n",
        "  print('response table updated')\n",
        "  question_override(dataset = 'dev_survey', stg_dateset = 'dev_stg', master = 'master_sheet')\n",
        "  load_questions_from_xl(data=xl)\n",
        "  insert_new_questions(dataset = 'dev_survey', stg_dateset = 'dev_stg')\n",
        "  print('question table updated')\n",
        "  load_raw_audience(dataset='dev_survey', response = response)\n",
        "  print('raw data loaded')\n",
        "  create_audience_table(dataset='dev_survey', stg_dateset='dev_stg')\n",
        "  print('audience table updated')\n",
        "  # archive_processed_file(filename)\n",
        "  # print('file is archived')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RkjwxhHy7ou"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efzG-vBe2Ap7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e91cd7-9ad5-4411-85e3-a1700844ec7d"
      },
      "source": [
        "create_table_for_dashboard(dataset='dev_survey')\n",
        "print('table for reporting updated')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhI5VHziidTg"
      },
      "source": [
        "# question_override(dataset = 'dev_survey', stg_dateset = 'dev_stg', master = 'master_sheet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfyZsWOYbjGs"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDfNmS5sSjEq"
      },
      "source": [
        "#### Run once"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3skE4t0Snk0",
        "outputId": "ac04b2b6-1a05-4915-ffc5-e4b4a8443e92"
      },
      "source": [
        "filename = 'dummydummy'\n",
        "xl = reading_file_from_bucket (filename, bucket_name = \"dummydummy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "vMDv0M_bXtbo",
        "outputId": "39d8f875-867f-4378-df00-fcc607f8d273"
      },
      "source": [
        "push_raw_data_to_bq(xl, stg_dateset = 'dev_stg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o1rNGeqbpGT",
        "outputId": "b0227ff5-1ead-4bcd-daad-dab28e22421b"
      },
      "source": [
        "load_questions_from_xl(data=xl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3RiM020XzOP"
      },
      "source": [
        "#### Test"
      ]
    }
  ]
}